{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "import torch \n",
    "from torch.utils.data import Dataset, Sampler, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "import csv\n",
    "from tqdm import tqdm_notebook as tqdm_notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calendar = pd.read_csv('calendar.csv')\n",
    "sales = pd.read_csv('sales_train_validation.csv')\n",
    "sales_2 = pd.read_csv('sales_train_evaluation.csv')\n",
    "prices = pd.read_csv('sell_prices.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plots ():\n",
    "\n",
    "    mean = sales.mean(axis = 0)\n",
    "\n",
    "    plt.plot(range(21),mean[0:21])\n",
    "    plt.show()\n",
    "\n",
    "    prices.groupby('wm_yr_wk').mean()\n",
    "\n",
    "    plt.plot(range(len(prices.groupby('wm_yr_wk').mean())),prices.groupby('wm_yr_wk').mean())\n",
    "    plt.show()\n",
    "\n",
    "#plots() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_dates ():\n",
    "    \n",
    "    # weeks\n",
    "    weeks = pd.get_dummies(calendar.wday)\n",
    "    weeks.columns = ['sat','sun','mon','tue', 'wen','thu','fri']\n",
    "\n",
    "    # months\n",
    "    month = pd.get_dummies(calendar.month)\n",
    "\n",
    "    # events\n",
    "    # calendar['event_name_1'].unique().size\n",
    "    # calendar['event_name_1'].unique()\n",
    "\n",
    "    event = pd.get_dummies(calendar.event_type_1)\n",
    "    event2 = pd.get_dummies(calendar.event_type_2)\n",
    "\n",
    "    for i in event2.columns:\n",
    "\n",
    "        event[i][event2[i]==1]=1\n",
    "\n",
    "    #event.iloc[1968]\n",
    "    #event.info()\n",
    "\n",
    "    global date_features\n",
    "\n",
    "    # Construct dates dataset\n",
    "    date_features = pd.concat([calendar['wm_yr_wk'], weeks, month, event, \n",
    "        calendar['snap_CA'],calendar['snap_TX'],calendar['snap_WI']], axis = 1)\n",
    "    date_features.head()\n",
    "\n",
    "    date_features.to_csv('date_features.csv' )\n",
    "\n",
    "#construct_dates ()\n",
    "\n",
    "def construct_items():\n",
    "\n",
    "    # items\n",
    "    dept = pd.get_dummies(sales.dept_id)\n",
    "    pd.get_dummies(sales.cat_id).head()\n",
    "    store = pd.get_dummies(sales.store_id)\n",
    "\n",
    "    # Construct items dataset\n",
    "    \n",
    "    global item_features\n",
    "    item_features = pd.DataFrame()\n",
    "    item_features['store_item'] = sales['store_id']+sales['item_id']\n",
    "    item_features = pd.concat([item_features , dept, store], axis = 1)\n",
    "    item_features.to_csv('item_features.csv' )\n",
    "\n",
    "#construct_items()\n",
    "\n",
    "# sales \n",
    "\n",
    "def construct_sales():\n",
    "    sales_data = sales_2.drop(['id', 'dept_id', 'cat_id', 'state_id','item_id','store_id'], axis = 1)\n",
    "    sales_data.to_csv('sales_data.csv' )\n",
    "\n",
    "#construct_sales()\n",
    "\n",
    "#prices\n",
    "\n",
    "def construct_prices ():\n",
    "    \n",
    "    global prices_data\n",
    "\n",
    "    prices['store_item'] = prices['store_id']+prices['item_id']\n",
    "    prices.drop(['store_id', 'item_id'],axis = 1, inplace = True)\n",
    "\n",
    "    #mean = prices.sell_price.mean()\n",
    "    #std = prices.sell_price.std()\n",
    "    #prices.sell_price = (prices.sell_price - mean)/ std\n",
    "\n",
    "    #min_max_scaler = preprocessing.MinMaxScaler()\n",
    "    #prices.sell_price = min_max_scaler.fit_transform(prices.sell_price.values.reshape(-1, 1))\n",
    "\n",
    "    prices_data = pd.pivot_table(prices, values='sell_price', \n",
    "            index=['store_item'], columns=['wm_yr_wk'])\n",
    "\n",
    "    index = item_features.store_item\n",
    "    prices_data = prices_data.reindex(index)\n",
    "\n",
    "    prices_data.loc['WI_3HOUSEHOLD_2_406'][prices_data.loc['WI_3HOUSEHOLD_2_406']>30] = 30\n",
    "    prices_data.loc['WI_1HOUSEHOLD_2_406'][prices_data.loc['WI_1HOUSEHOLD_2_406']>30] = 30\n",
    "    prices_data.loc['WI_2HOUSEHOLD_2_406'][prices_data.loc['WI_2HOUSEHOLD_2_406']>30] = 30\n",
    "    prices_data.loc['TX_1HOUSEHOLD_2_466'][prices_data.loc['TX_1HOUSEHOLD_2_466']>30] = 30\n",
    "    prices_data.loc['TX_1HOUSEHOLD_2_178'][prices_data.loc['TX_1HOUSEHOLD_2_178']>30] = 30\n",
    "    prices_data.loc['WI_2HOUSEHOLD_2_250'][prices_data.loc['WI_2HOUSEHOLD_2_250']>30] = 30\n",
    "\n",
    "    prices_data.to_csv('prices_data.csv' )\n",
    "\n",
    "#construct_prices ()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "calendar_path = 'date_features.csv'\n",
    "items_path = 'item_features.csv'\n",
    "sales_path = 'sales_data.csv'\n",
    "prices_path = 'prices_data.csv'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess():\n",
    "\n",
    "    calendar = pd.read_csv(calendar_path, index_col = 0)\n",
    "    items = pd.read_csv(items_path, index_col = 0)\n",
    "    sales = pd.read_csv(sales_path, index_col = 0)\n",
    "    prices = pd.read_csv(prices_path,index_col = 0)\n",
    "\n",
    "    price_std = prices.stack().std()  \n",
    "    price_mean = prices.mean().mean()\n",
    "\n",
    "    sales_std = sales.stack().std() \n",
    "    sales_mean = prices.mean().mean()\n",
    "\n",
    "    prices_standar = (prices - price_mean) / price_std\n",
    "    sales_standar = (sales - sales_mean) / sales_std\n",
    "\n",
    "    return calendar,items, prices_standar, sales_standar, sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Data (Dataset):\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        self.calendar, self.items, self.prices, self.sales, self.sales_target = preprocess()\n",
    "\n",
    "    def construct_index_train (self):\n",
    "\n",
    "      \n",
    "        date = []\n",
    "        item = []\n",
    "\n",
    "        for i in tqdm(range (67)):\n",
    "\n",
    "            if i < 47:\n",
    "            \n",
    "                date_idx = 9 + i * 28 + 27\n",
    "                week = (self.calendar.iloc[date_idx].wm_yr_wk)\n",
    "\n",
    "                for j in range( len (self.prices)):\n",
    "\n",
    "                    if not np.isnan(self.prices.iloc[j].loc[str(week)]) :\n",
    "\n",
    "                        date.append(i)\n",
    "                        item.append(j)\n",
    "            \n",
    "            else:\n",
    "                for j in range( len (self.prices)):\n",
    "\n",
    "                    date.append(i)\n",
    "                    item.append(j)\n",
    "       \n",
    "    \n",
    "        idxzip = pd.DataFrame(data={\"date\": date, \"item\": item})\n",
    "        idxzip.to_csv(\"./idxzip.csv\", sep=',',index=False)\n",
    "\n",
    "        return zip(date, item)\n",
    "\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "        index1, item_idx = index\n",
    "\n",
    "        date_idx = 9 + index1 * 28  \n",
    "        dates = self.calendar.iloc[date_idx : date_idx + 28].copy()\n",
    "\n",
    "        item = self.items.iloc[item_idx].copy()\n",
    "        store_item = item['store_item']\n",
    "\n",
    "        weeks = dates['wm_yr_wk'].unique()\n",
    "        dates['price'] = \"\"\n",
    "        dates['not_sold'] = pd.Series(np.zeros((28)), index=dates.index)\n",
    "\n",
    "        for week in weeks:\n",
    "\n",
    "            weekprice = self.prices.loc[store_item,str(week)]\n",
    "            \n",
    "            if np.isnan(weekprice):\n",
    "\n",
    "                dates.loc[dates.wm_yr_wk == week, \n",
    "                    'price'] = 8\n",
    "                dates.loc[dates.wm_yr_wk == week, \n",
    "                    'not_sold'] = 1\n",
    "            else:\n",
    "                dates.loc[dates.wm_yr_wk == week, \n",
    "                    'price'] = weekprice\n",
    "\n",
    "        past_sales = self.sales.iloc[item_idx , date_idx - 9 : date_idx].mean()\n",
    "        \n",
    "        X_dates = dates.drop(['wm_yr_wk'], axis=1).values.astype('float32') \n",
    "\n",
    "        X_item = item[1:].values\n",
    "        X_item = np.insert(X_item,0,past_sales).astype('float32') \n",
    "\n",
    "        Y = self.sales_target.iloc[item_idx , date_idx : date_idx + 28].values.astype('float32') \n",
    "        \n",
    "\n",
    "        return X_dates, X_item, Y\n",
    "\n",
    "    def __len__ (self):\n",
    "\n",
    "        return ((len(self.calendar)-9)/28) * len(self.items)\n",
    "\n",
    "dataset = Data ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_index = dataset.construct_index_train()\n",
    "\n",
    "#idxdf = pd.read_csv('idxzip.csv')\n",
    "#train_index = zip(idxdf.date.tolist() , idxdf.item.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampler / Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampler(Sampler):\n",
    "    \n",
    "    def __init__(self, train, test = False, val = False, train_index = None):\n",
    "\n",
    "        self.items = 30490\n",
    "        self.train = train\n",
    "        self.test = test\n",
    "        self.val = val\n",
    "\n",
    "        if self.train == True:\n",
    "            self.dates = range(67)\n",
    "            self.lenght = self.items * 67 \n",
    "            self.index = train_index\n",
    "        elif self.test == True:\n",
    "            self.dates = range(67,68)\n",
    "            self.lenght = self.items  \n",
    "\n",
    "        elif self.val == True:\n",
    "            self.dates = range(68, 69)\n",
    "            self.lenght = self.items \n",
    "        else:\n",
    "            self.dates = range(69, 70)\n",
    "            self.lenght = self.items \n",
    "\n",
    "    def __iter__(self):\n",
    "        \n",
    "        if self.train == False:\n",
    "\n",
    "            date = []\n",
    "            item = []\n",
    "            for i in self.dates:\n",
    "                for j in range(self.items):\n",
    "\n",
    "                        date.append(i)\n",
    "                        item.append(j)\n",
    "\n",
    "            return iter(zip(date, item))\n",
    "        \n",
    "        else :\n",
    "\n",
    "            return iter(self.index)\n",
    "\n",
    "    def __len__(self):\n",
    "        \n",
    "        return self.lenght"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_sampler = Sampler(train = True, train_index = train_index) \n",
    "test_sampler = Sampler(train = False, test = True)\n",
    "val_sampler = Sampler (train = False, val = True)\n",
    "pred_sampler = Sampler (train = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader  = DataLoader(dataset, \n",
    "    batch_size=4, sampler=train_sampler, shuffle=False, num_workers = 3, drop_last=True)\n",
    "\n",
    "test_loader  = DataLoader(dataset, \n",
    "    batch_size=1, sampler=test_sampler, shuffle=False, num_workers = 3)\n",
    "\n",
    "val_loader  = DataLoader(dataset, \n",
    "    batch_size=1, sampler=val_sampler, shuffle=False, num_workers = 3)\n",
    "\n",
    "pred_loader  = DataLoader(dataset, \n",
    "    batch_size=1, sampler=pred_sampler, shuffle=False, num_workers = 3)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "\n",
    "    def __init__(self, dates_dim, item_dim, lstm_hidden, hidden1,  \n",
    "            seq_len = 28, lstm_num_layers = 1, LSTM_dropout = 0):\n",
    "\n",
    "        super(Network, self).__init__()\n",
    "\n",
    "        self.lstm = nn.LSTM(dates_dim, lstm_hidden, lstm_num_layers, \n",
    "                        batch_first = True, dropout = LSTM_dropout )\n",
    "        self.linear_out = nn.Linear(lstm_hidden, 1)\n",
    "\n",
    "        self.linear_1 = nn.Linear(item_dim, hidden1)\n",
    "        self.linear_h0 = nn.Linear(hidden1, lstm_hidden)\n",
    "        self.linear_c0 = nn.Linear(hidden1, lstm_hidden)\n",
    "\n",
    "        self.sigm = nn.Sigmoid()\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "\n",
    "    def forward (self, x_dates, x_item):\n",
    "\n",
    "        x_item = self.sigm(self.linear_1(x_item)) # (batch, hidden1)\n",
    "        h0 = self.sigm(self.linear_h0(x_item))    # (batch, lstm _hidden)\n",
    "        c0 = self.sigm(self.linear_c0(x_item))    # (batch, lstm _hidden)\n",
    "\n",
    "        h0 = torch.unsqueeze(h0, 0)               # (1, batch, lstm _hidden)\n",
    "        c0 = torch.unsqueeze(c0, 0)               # (1, batch, lstm _hidden)\n",
    "\n",
    "        output, (hn, cn) = self.lstm(x_dates, (h0, c0)) # (batch, 28, lstm_hidden)\n",
    "        output = self.relu(self.linear_out(output))    # (batch, 28, 1)\n",
    "        output = torch.squeeze(output, 2)            # (batch, 28)\n",
    "    \n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Network (28, 18, 60, 25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 4 \n",
    "prin = 80000\n",
    "test = 160000\n",
    "\n",
    "avg_test_loss = []\n",
    "min_test_loss = 8.81\n",
    "\n",
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train ():\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        for i, data in tqdm_notebook(enumerate(train_loader)):\n",
    "\n",
    "            x_date, x_item, target = data\n",
    "            model.zero_grad()\n",
    "            pred =  model(x_date, x_item)\n",
    "            loss = criterion (pred, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if i % prin == 0:\n",
    "\n",
    "                print ('\\ni:', i , '  Epoch:', epoch)\n",
    "                print ('\\nLoss: ', loss.item())\n",
    "                print (data[0].shape, data[1].shape, data[2].shape)\n",
    "\n",
    "                if (i % test == 0 and i != 0):\n",
    "\n",
    "                    model.eval()\n",
    "                    test_loss = []\n",
    "                    for testdata in test_loader:\n",
    "\n",
    "                        x_date_test, x_item_test, target_test = testdata\n",
    "                        pred_test =  model(x_date_test, x_item_test)\n",
    "                        loss_test = criterion (pred_test, target_test)\n",
    "                        test_loss.append(loss_test.item())\n",
    "                    \n",
    "                    avg_test_loss.append(np.mean(test_loss))\n",
    "                    model.train()\n",
    "                    print ('\\n\\n\\nAvg test loss:  ', np.mean(test_loss))\n",
    "                    print ('\\n', avg_test_loss, '\\n\\n\\n')\n",
    "                    if np.mean(test_loss) <= min_test_loss:\n",
    "\n",
    "                        torch.save(model.state_dict(), './state_dict8.pt')\n",
    "                        valid_loss_min = np.mean(test_loss)\n",
    "\n",
    "    model.eval()\n",
    "    test_loss = []\n",
    "    for testdata in test_loader:\n",
    "\n",
    "        x_date_test, x_item_test, target_test = testdata\n",
    "        pred_test =  model(x_date_test, x_item_test)\n",
    "        loss_test = criterion (pred_test, target_test)\n",
    "        test_loss.append(loss_test.item())\n",
    "    \n",
    "    avg_test_loss.append(np.mean(test_loss))\n",
    "    print ('\\n\\n\\nAvg test loss:  ', np.mean(test_loss))\n",
    "    print ('\\n', avg_test_loss, '\\n\\n\\n')\n",
    "    if np.mean(test_loss) <= min_test_loss:\n",
    "\n",
    "        torch.save(model.state_dict(), './state_dict8.pt')\n",
    "        valid_loss_min = np.mean(test_loss)\n",
    "\n",
    "    print ('\\n ---------------------------\\n test loss :', avg_test_loss)\n",
    "\n",
    "train()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.load_state_dict(torch.load('./state_dict8.pt'))\n",
    "\n",
    "model.eval()\n",
    "val_loss = []\n",
    "\n",
    "index = pd.Index(pd.read_csv('subm_idx.csv', header=None, index_col = 0).iloc[:,0])\n",
    "submission = pd.DataFrame(index = index, columns = ['F1','F2','F3','F4','F5','F6','F7','F8','F9','F10',\n",
    "                                        'F11','F12','F13','F14','F15','F16','F17','F18','F19',\n",
    "                                        'F20','F21','22','F23','F24','F25','F26','F27','F28'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, valdata in tqdm_notebook(enumerate(val_loader)):\n",
    "\n",
    "    x_date_val, x_item_val, target_val = valdata\n",
    "    pred_val =  model(x_date_val, x_item_val)\n",
    "    loss_val = criterion (pred_val, target_val)\n",
    "    val_loss.append(loss_val.item())\n",
    "    submission.iloc[i] = pred_val.detach().numpy()\n",
    "\n",
    "    if i % 3000 == 0:\n",
    "\n",
    "        print (np.mean(val_loss))\n",
    "\n",
    "print ('val loss : ', np.mean(val_loss))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, preddata in tqdm_notebook(enumerate(pred_loader)):\n",
    "\n",
    "    x_date_pred, x_item_pred, target_pred = preddata\n",
    "    pred_pred =  model(x_date_pred, x_item_pred)\n",
    "    submission.iloc[i+30490] = pred_pred.detach().numpy()\n",
    "\n",
    "submission.to_csv('submission8.csv' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Test MSEloss :', avg_test_loss)\n",
    "print ('Validation MSEloss : ', np.mean(val_loss))\n",
    "print ('---------------------------')\n",
    "print ('Test RMSEloss :', np.sqrt(avg_test_loss))\n",
    "print ('Validation RMSEloss : ', np.sqrt(np.mean(val_loss)))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
